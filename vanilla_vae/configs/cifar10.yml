# VAE config used for VAE training
vae:
  data:
    root: 'data/'
    data_name: "cifar10"
    num_clusters_data: 10
    image_size: 32
    n_channels: 3
    hflip: False

  model:  # VAE specific params. Check the `main/models/vae.py`
    enc_block_config : "32x1,32d2,32t16,16x1,16d2,16t8,8x1,8d2,8t4,4x1"
    enc_channel_config: "32:32,16:128,8:512,4:1024"
    dec_block_config: "4x2,4u2,4t8,8x2,8u2,8t16,16x2,16u2,16t32,32x2"
    dec_channel_config: "32:32,16:128,8:512,4:1024"

  training:   # Most of these are same as explained above but for VAE training
    seed: 0
    fp16: False
    batch_size: 256
    epochs: 1500
    log_step: 1
    device: "gpu:0"
    chkpt_interval: 1
    optimizer: "Adam"
    lr: 0.0001
    restore_path: ""
    results_dir: "vanilla_vae/results/"
    workers: 1
    chkpt_prefix: ""
    alpha: 1.0   # The beta value in beta-vae. 
    
  globals:
    seed: 42                    # Random seed for reproducibility


## DiffuseVAE specific parameters --------------------------------------------------------------------------------
ddpm:
  # Parameters for data loading, similar to the TreeVAE configs
  data:
    data_name: 'cifar10'    # Dataset name
    num_clusters_data: 10   # Number of clusters/classes in the dataset
    image_size: 32          # Image resolution (32 for CIFAR10)
    inp_channels: 3         # Num input channels
    norm: True              # Whether to scale data between [-1, 1]
    ddpm_latent_path: ""    # If sharing DDPM latents between diffusevae samples, path to .pt tensor containing latent codes

  globals:
    seed: 42                # Random seed for reproducibility

  # UNet specific params. Check the DDPM implementation for details on these
  model:
    dim: 128                    # UNet base dimension for channels
    attn_resolutions: "16, 8"   # Attention resolutions for the UNet
    n_residual: 2               # Number of residual blocks per level in UNet between the resolutions
    dim_mults: "1,2,2,2"        # Multiplier for Nb of channels for each level in UNet
    dropout: 0.1                # Dropout rate
    n_heads: 8                  # Number of attention heads
    beta1: 0.0001
    beta2: 0.02
    n_timesteps: 1000           # Number of diffusion timesteps

  # Training parameters for the DDPM part given TreeVAE model
  training:
    # paths
    vae_chkpt_path: ''          # TreeVAE checkpoint path
    results_dir: ''             # Directory to store the DDPM checkpoint in
    chkpt_prefix: "vae"         # prefix appended to the checkpoint name
    restore_path: ""            # Checkpoint restore path
    chkpt_interval: 1           # Number of epochs between two checkpoints

    # Model-specific training parameters
    use_ema: True         # Whether to use EMA (Improves sample quality)
    ema_decay: 0.9999     # EMA decay rate
    z_cond: False         # Whether to condition UNet on vae latent
    z_dim: None           # Dimensionality of the vae latent --> 1 for leaf index
    type: 'form1'         # DiffuseVAE formulation type, One of ['form1', 'form2', 'uncond'].
    cfd_rate: 0.0         # Conditioning signal dropout rate as in Classifier-free guidance

    # General training parameters
    batch_size: 64        # Batch size
    epochs: 1000          # Max number of epochs
    n_anneal_steps: 5000  # number of warmup steps
    loss: "l2"            # Diffusion loss type. Among ['l2', 'l1']
    optimizer: "Adam"     # Optimizer
    lr: 0.0002            # Learning rate
    grad_clip: 1.0        # gradient clipping threshold

  # Parameters for evaluation of the reconstructions and generations
  evaluation:
    # paths and saving
    chkpt_path: ''                        # DiffuseVAE checkpoint path
    save_path: 'vanilla_vae/results/'     # Path to write samples to
    save_vae: True                        # Whether to save VAE
    save_mode: 'image'                    # Whether to save samples as .png or .npy. One of ['image', 'numpy']
    sample_prefix: ""                     # Prefix used in naming when saving samples to disk
    eval_mode: ''        # Evaluation mode. One of ['sample', 'sample_all_leaves', 'recons', 'recons_all_leaves']

    # Model-specific evaluation parameters
    guidance_weight: 0.0  # Guidance weight during sampling if using Classifier free guidance

    # General evaluation parameters for sampling
    resample_strategy: "spaced"       # Whether to use spaced or truncated sampling. Use 'truncated' if sampling for the entire 1000 steps
    skip_strategy: "quad"             # Skipping strategy to use if `resample_strategy=spaced`. Can be ['uniform', 'quad'] as in DDIM
    sample_method: "ddim"             # Sampling backend. Can be ['ddim', 'ddpm']
    sample_from: "target"             # Whether to sampling from the (non)-EMA model. Can be ['source', 'target']
    variance: "fixedlarge"            # DDPM variance to use when using DDPM. Can be ['fixedsmall', 'fixedlarge']
    temp: 1.0                         # Temperature sampling factor in DDPM latents
    n_samples: 10000                  # Number of samples to generate
    n_steps: 100                      # Number of reverse process steps to use during sampling. Typically [0-100] for DDIM and T=1000 for DDPM
