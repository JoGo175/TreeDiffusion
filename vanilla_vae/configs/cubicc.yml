# VAE config used for VAE training
vae:
  data:
    root: 'data/'
    data_name: "cubicc"
    num_clusters_data: 10
    image_size: 64
    n_channels: 3
    hflip: False

  model:  # VAE specific params. Check the `main/models/vae.py`
    enc_block_config : "64x2,64d2,64t32,32x2,32d2,32t16,16x2,16d2,16t8,8x2,8d2,8t4,4x2"
    enc_channel_config: "64:32,32:128,16:256,8:512,4:1024"
    dec_block_config: "4x3,4u2,4t8,8x3,8u2,8t16,16x3,16u2,16t32,32x3,32u2,32t64,64x3"
    dec_channel_config: "64:32,32:128,16:256,8:512,4:1024"

  training:   # Most of these are same as explained above but for VAE training
    seed: 0
    fp16: False
    batch_size: 128
    epochs: 500
    log_step: 1
    device: "gpu:0"
    chkpt_interval: 1
    optimizer: "Adam"
    lr: 0.0001
    restore_path: ""
    results_dir: "vanilla_vae/results/"
    workers: 1
    chkpt_prefix: ""
    alpha: 1.0   # The beta value in beta-vae. 
        
  globals:
    seed: 42                    # Random seed for reproducibility